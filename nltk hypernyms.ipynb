{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red color or pigment; the chromatic color resembling the hue of blood\n",
      "[Synset('cardinal.n.03'), Synset('cerise.n.01'), Synset('chrome_red.n.01'), Synset('crimson.n.01'), Synset('dark_red.n.01'), Synset('purplish_red.n.01'), Synset('sanguine.n.01'), Synset('scarlet.n.01'), Synset('turkey_red.n.01')]\n"
     ]
    }
   ],
   "source": [
    "word = 'red.n.01'\n",
    "s = wn.synset(word)\n",
    "print(s.definition())\n",
    "print(s.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-482ae0d3e4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "off = []\n",
    "for line in open(\"alignment.tsv\",encoding = \"UTF-8\"):\n",
    "    l = line.split('\\t')\n",
    "    offset = int(l[0])\n",
    "    if offset not in off:\n",
    "        off.append(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('red.n.01'), Synset('red.n.02'), Synset('bolshevik.n.01'), Synset('loss.n.06'), Synset('red.s.01'), Synset('crimson.s.02'), Synset('crimson.s.03')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets('red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "table = {ord(c): None for c in string.punctuation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms = []\n",
    "\n",
    "for h in s.hypernyms():\n",
    "    temp = []\n",
    "    hd = h.definition().split()\n",
    "    for h in hd:\n",
    "        temp.append(h.translate(table))\n",
    "    definition = \" \".join(temp)\n",
    "    hypernyms.append(definition)\n",
    "\n",
    "document1 = \" \".join(hypernyms)\n",
    "print(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyponyms = []\n",
    "\n",
    "for h in s.hyponyms():\n",
    "    temp = []\n",
    "    hd = h.definition().split()\n",
    "    for h in hd:\n",
    "        temp.append(h.translate(table))\n",
    "    definition = \" \".join(temp)\n",
    "    hyponyms.append(definition)\n",
    "    \n",
    "document2 = \" \".join(hyponyms)\n",
    "print(document2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.append(document1)\n",
    "corpus.append(document2)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_Dict = defaultdict(lambda: list())\n",
    "vectorizer = TfidfVectorizer()\n",
    "try:\n",
    "    print(corpus)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    print(tfidf.shape)\n",
    "\n",
    "    words = vectorizer.get_feature_names()\n",
    "    #print(words)\n",
    "    for i in range(len(corpus)):\n",
    "        print('----Document %d----' % (i))\n",
    "        for j in range(len(words)):\n",
    "            print(\"words[j] \", words[j],\" tfidf \", tfidf[i,j])\n",
    "            if tfidf[i,j] > 0.25:\n",
    "                tf_idf_Dict[word].append(words[j])\n",
    "except:\n",
    "    print(word,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_Dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = s.definition()\n",
    "de = d.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = []\n",
    "for d in de:\n",
    "    definition.append(d.translate(table))\n",
    "    \n",
    "print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in definition:\n",
    "    if d in tf_idf_Dict[word]:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5174653\n"
     ]
    }
   ],
   "source": [
    "ss = wn.synset('right.n.01')\n",
    "offset = str(ss.offset())\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('discussion.n.02')"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset_from_pos_and_offset('n', 7140659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_withgooglenews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar('plant', topn = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'plant'\n",
    "#print(model.most_similar(positive=['plant','flora'], topn = 10))\n",
    "for word in model.most_similar(positive=['plant','flora'], topn = 10):\n",
    "    if w not in str(word[0]).lower():\n",
    "        print(str(word[0]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['plant','flora','botany'], topn = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['plant','organism','crop'], topn = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.15\n",
    "threshold2 = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "temp_alignment = defaultdict(lambda:list())\n",
    "for line in open(\"alignment.tsv\",encoding = \"UTF-8\"):\n",
    "    l = line.split('\\t')\n",
    "    offset = int(l[0])\n",
    "    title = l[1].lower()\n",
    "    p1 = float(l[2])\n",
    "    p2 = float(l[3])\n",
    "    temp_alignment[offset].append('')\n",
    "    if(p1 < threshold1 and p2 < threshold2):continue\n",
    "    index1 = title.find('(')\n",
    "    if index1 > 0:\n",
    "        index2 = title.find(')')\n",
    "        title = title[index1+1:index2]\n",
    "        #print(title)\n",
    "    if title not in model.wv.vocab:continue\n",
    "    temp_alignment[offset].append(title)\n",
    "    #print(offset,title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['botany', 'flora', 'plant']\n"
     ]
    }
   ],
   "source": [
    "print(temp_alignment[17222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in temp_alignment.keys():\n",
    "    if('' in temp_alignment[offset]):\n",
    "        temp_alignment[offset] = list(filter(lambda x: x != '', temp_alignment[offset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x126920730>, {3553: []})\n"
     ]
    }
   ],
   "source": [
    "print(temp_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = defaultdict(lambda:list())\n",
    "for offset in temp_alignment.keys():\n",
    "    sys = wn.synset_from_pos_and_offset('n', offset)\n",
    "    alignment[sys] = []\n",
    "    for title in temp_alignment[offset]:\n",
    "        s = sys.name().split('.')[0]\n",
    "        #print(s)\n",
    "        if str(s) not in str(title):\n",
    "            alignment[sys].append(str(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x1278b7840>, {Synset('whole.n.02'): []})\n"
     ]
    }
   ],
   "source": [
    "print(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "lemma_dict = defaultdict(lambda:list())\n",
    "for sys in alignment.keys():\n",
    "    s = sys.name().split('.')[0]\n",
    "    # Those lemma lower than 1 will be filter out\n",
    "    #lemma_dict[sys] = []\n",
    "    for lemma in sys.lemmas():\n",
    "        l = str(lemma.name())\n",
    "        if s not in l:\n",
    "            if l not in model.wv.vocab:continue\n",
    "            lemma_dict[sys].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x126920a60>, {Synset('whole.n.02'): ['unit']})\n"
     ]
    }
   ],
   "source": [
    "print(lemma_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_dict = defaultdict(lambda: '')\n",
    "for sys in lemma_dict.keys():\n",
    "    definition_dict[sys] = sys.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-473-cebdd1f7dfc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefinition_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog.n.01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_name'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(definition_dict[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hypernyms and hyponyms ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "hypernyms_dict = defaultdict(lambda:list())\n",
    "for sys in lemma_dict.keys():\n",
    "    hypernyms_dict[sys] = []\n",
    "    for hy in sys.hypernyms():\n",
    "        hyper = str(hy.name().split('.')[0])\n",
    "        if hyper not in model.wv.vocab:continue\n",
    "        hypernyms_dict[sys].append(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(hypernyms_dict[wn.synsets('plant')[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "hyponyms_dict = defaultdict(lambda:list())\n",
    "for sys in lemma_dict.keys():\n",
    "    hyponyms_dict[sys] = []\n",
    "    for ho in sys.hyponyms():\n",
    "        hypo = str(ho.name().split('.')[0])\n",
    "        if hypo not in model.wv.vocab:continue\n",
    "        hyponyms_dict[sys].append(hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(hyponyms_dict[wn.synsets('plant')[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInWord2Vec(input_word, input_list):\n",
    "    result = []\n",
    "    for word in model.most_similar(positive=input_list, topn = 5):\n",
    "        if input_word not in str(word[0]).lower():\n",
    "            result.append(str(word[0]).lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "result_dict = defaultdict(lambda: list())\n",
    "for sys in lemma_dict.keys():\n",
    "    input_word = sys.name().split('.')[0]\n",
    "    key = sys.name()\n",
    "    if input_word not in model.wv.vocab:continue\n",
    "    # combination 1 :lemma*1 + original word\n",
    "    for lemma in lemma_dict[sys]:\n",
    "        result_dict[key].append(['c1'] + searchInWord2Vec(input_word, [lemma] + [input_word]))\n",
    "    # combination 2: lemma*2 + original word\n",
    "    com2 = list(itertools.combinations(lemma_dict[sys],2))\n",
    "    for c2 in com2:\n",
    "        result_dict[key].append(['c2'] + searchInWord2Vec(input_word, list(c2) + [input_word]))\n",
    "    # combination 3: lemma*3 + original word\n",
    "    com3 = list(itertools.combinations(lemma_dict[sys],3))\n",
    "    for c3 in com3:\n",
    "        result_dict[key].append(['c3'] + searchInWord2Vec(input_word, list(c3) + [input_word]))\n",
    "    # combination 4: hyper + lemma + original word\n",
    "    for lemma in lemma_dict[sys]:\n",
    "        for hy in hypernyms_dict[sys]:\n",
    "            result_dict[key].append(['c4'] + searchInWord2Vec(input_word, [lemma] + [input_word] + [hy]))\n",
    "    # combination 5: hyper + original word\n",
    "    for hy in hypernyms_dict[sys]:\n",
    "        result_dict[key].append(['c5'] + searchInWord2Vec(input_word, [input_word] + [hy]))\n",
    "    # combination 6: hypo + lemma + original word\n",
    "    for lemma in lemma_dict[sys]:\n",
    "        for ho in hyponyms_dict[sys]:\n",
    "            result_dict[key].append(['c6'] + searchInWord2Vec(input_word, [lemma] + [input_word] + [ho]))\n",
    "    # combination 7: hypo + original word\n",
    "    for ho in hyponyms_dict[sys]:\n",
    "            result_dict[key].append(['c7'] + searchInWord2Vec(input_word, [input_word] + [ho]))\n",
    "    # combination 8: wiki title*1 + original word\n",
    "    for title in alignment[sys]:\n",
    "        result_dict[key].append(['c8'] + searchInWord2Vec(input_word, [title] + [input_word]))\n",
    "    # combination 9: wiki title*2 + original word\n",
    "    com9 = list(itertools.combinations(alignment[sys],2))\n",
    "    for c9 in com9:\n",
    "        result_dict[key].append(['c9'] + searchInWord2Vec(input_word, list(c9) + [input_word]))\n",
    "    # combination 10: wiki title*3 + original word\n",
    "    com10 = list(itertools.combinations(alignment[sys],2))\n",
    "    for c10 in com10:\n",
    "        result_dict[key].append(['c10'] + searchInWord2Vec(input_word, list(c10) + [input_word]))\n",
    "    # combination 11: wiki title + lemma + original word\n",
    "    for lemma in lemma_dict[sys]:\n",
    "        for title in alignment[sys]:\n",
    "            result_dict[key].append(['c11'] + searchInWord2Vec(input_word, [lemma] + [input_word] + [title]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hello.json','w') as fp:\n",
    "    json.dump(result_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['c1', 'factory', 'lime_kiln', 'bio_digester'], ['c6', 'factory', 'brewery', 'bottling', 'lime_kiln'], ['c6', 'distillery', 'factory', 'breweries'], ['c6', 'brewery', 'lime_kiln', 'distilleries'], ['c6', 'factories', 'manufacturing', 'toyota_bodine_aluminum'], ['c6', 'factory', 'hydroponic_tomato', 'unopened_buds', 'parsley_chervil'], ['c6', 'factory', 'canning_factory', 'cannery'], ['c6', 'bpd_refinery', 'refineries', 'refinery', 'motiva_refinery'], ['c6', 'lime_kiln', 'biodigester', 'bio_digester', 'papermill'], ['c6', 'aluminum_smelter', 'copper_smelter', 'smelting', 'lime_kiln'], ['c7', 'factory', 'brewery', 'bottling', 'distillery'], ['c7', 'distillery', 'factory', 'breweries'], ['c7', 'brewery', 'distilleries', 'whiskey_distillery'], ['c7', 'factories', 'manufacturing', 'mill'], ['c7', 'factory', 'unopened_buds', 'parsley_chervil'], ['c7', 'factory', 'cannery', 'peach_orchard'], ['c7', 'refineries', 'refinery', 'bpd_refinery', 'motiva_refinery'], ['c7', 'factory', 'lime_kiln', 'papermill', 'containerboard_mill'], ['c7', 'aluminum_smelter', 'copper_smelter', 'smelting', 'smelters']]\n"
     ]
    }
   ],
   "source": [
    "print(result_dict['plant.n.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-511-6848b9ef1e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "for sys in result_dict.keys():\n",
    "    result = []\n",
    "    for l in result_dict[l]:\n",
    "        result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 4]\n",
      "[1, 3, 4]\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "li = list(itertools.combinations([1,2,3,4],3))\n",
    "for l in li:\n",
    "    print(list(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys Synset('abstraction.n.06')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('thing.n.12')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('object.n.01')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('whole.n.02')\n",
      "lemma ['unit']\n",
      "[]\n",
      "sys Synset('congener.n.03')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('living_thing.n.01')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('organism.n.01')\n",
      "lemma ['being']\n",
      "[]\n",
      "sys Synset('benthos.n.02')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('dwarf.n.03')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('heterotroph.n.01')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('parent.n.02')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('life.n.10')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('cell.n.02')\n",
      "lemma []\n",
      "[]\n",
      "sys Synset('causal_agent.n.01')\n",
      "lemma ['cause']\n",
      "[]\n",
      "sys Synset('person.n.01')\n",
      "lemma ['individual', 'someone', 'somebody', 'mortal', 'soul']\n",
      "[('individual', 'someone'), ('individual', 'somebody'), ('individual', 'mortal'), ('individual', 'soul'), ('someone', 'somebody'), ('someone', 'mortal'), ('someone', 'soul'), ('somebody', 'mortal'), ('somebody', 'soul'), ('mortal', 'soul')]\n",
      "['individual', 'someone']\n",
      "['somebody', 'person', 'somebody_else', 'anyone', 'individuals']\n",
      "['individual', 'somebody']\n",
      "['someone', 'somebody_else', 'anybody', 'person', 'everybody']\n",
      "['individual', 'mortal']\n",
      "['earthly', 'seawalls_protect', 'unspeakably_evil', 'mortal_beings', 'omnipotent_deity']\n",
      "['individual', 'soul']\n",
      "['sweetest_gentlest_kind', 'seawalls_protect', 'souls', 'siren_joss_stone', 'innumerable_facets']\n",
      "['someone', 'somebody']\n",
      "['somebody_else', 'anybody', 'someone', 'anyone', 'somebody']\n",
      "['someone', 'mortal']\n",
      "['somebody', 'somebody_else', 'anyone', 'someone', 'anybody']\n",
      "['someone', 'soul']\n",
      "['somebody', 'someone', 'somebody_else', 'sweetest_gentlest_kind', 'anyone']\n",
      "['somebody', 'mortal']\n",
      "['someone', 'somebody_else', 'anybody', 'nobody', 'guy']\n",
      "['somebody', 'soul']\n",
      "['someone', 'somebody_else', 'anybody', 'sweetest_gentlest_kind', 'somebody']\n",
      "['mortal', 'soul']\n",
      "['souls', 'sweetest_gentlest_kind', 'neshama', 'eternal', 'deepest_longings']\n",
      "sys Synset('animal.n.01')\n",
      "lemma ['beast', 'brute', 'creature', 'fauna']\n",
      "[('beast', 'brute'), ('beast', 'creature'), ('beast', 'fauna'), ('brute', 'creature'), ('brute', 'fauna'), ('creature', 'fauna')]\n",
      "['beast', 'brute']\n",
      "['beasts', 'monster', 'brutish', 'beastly', 'brutes']\n",
      "['beast', 'creature']\n",
      "['beasts', 'creatures', 'monster', 'beastie', 'critter']\n",
      "['beast', 'fauna']\n",
      "['beasts', 'creature', 'creatures', 'mammal', 'felids']\n",
      "['brute', 'creature']\n",
      "['beast', 'creatures', 'beasts', 'mythological_beast', 'monster']\n",
      "['brute', 'fauna']\n",
      "['skink', 'felids', 'canis_lupus', 'creatures', 'hyrax']\n",
      "['creature', 'fauna']\n",
      "['creatures', 'mammal', 'skink', 'species', 'lizard']\n"
     ]
    }
   ],
   "source": [
    "for sys in alignment.keys():\n",
    "    print('sys', sys)\n",
    "    print('lemma', lemma_dict[sys])\n",
    "    com2 = list(itertools.combinations(lemma_dict[sys],2))\n",
    "    print(com2)\n",
    "    for c in com2:\n",
    "        print(list(c))\n",
    "        print(searchInWord2Vec(input_word, list(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('abstraction.n.01.abstraction'), Lemma('abstraction.n.01.abstract')]\n",
      "[Lemma('abstraction.n.02.abstraction')]\n",
      "[Lemma('abstraction.n.03.abstraction'), Lemma('abstraction.n.03.generalization'), Lemma('abstraction.n.03.generalisation')]\n",
      "[Lemma('abstraction.n.04.abstraction')]\n",
      "[Lemma('abstractedness.n.01.abstractedness'), Lemma('abstractedness.n.01.abstraction')]\n",
      "[Lemma('abstraction.n.06.abstraction'), Lemma('abstraction.n.06.abstract_entity')]\n"
     ]
    }
   ],
   "source": [
    "word = 'abstraction'\n",
    "s = wn.synsets(word)\n",
    "for i in s:\n",
    "    print(i.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
